{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Do initial setup. This involves the next four code-blocks:\n",
    "1. Import required modules:\n",
    "        * pyspark – allows for running on multiple nodes\n",
    "        * dxpy – allows for querying DNANexus-specific files\n",
    "        * dxdata – allows for querying DNANexus-specific databases\n",
    "        * pandas – dataframe manipulation\n",
    "2. Connect to the pyspark cluster to be able to pull UK Biobank data, structured in a mysql-like database\n",
    "3. Use dxpy to find the database ‘file’ in our current project.\n",
    "4. Load the database into this instance with the dxdata package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import math\n",
    "import csv"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dispensed_dataset_id = dxpy.find_one_data_object(typename = \"Dataset\",\n",
    "                                                name = \"app*.dataset\", folder = \"/\",\n",
    "                                                name_mode = \"glob\") [\"id\"]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = dxdata.load_dataset(id = dispensed_dataset_id)\n",
    "dataset.entities"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<Entity \"participant\">,\n <Entity \"death\">,\n <Entity \"death_cause\">,\n <Entity \"hesin\">,\n <Entity \"hesin_critical\">,\n <Entity \"hesin_delivery\">,\n <Entity \"hesin_diag\">,\n <Entity \"hesin_maternity\">,\n <Entity \"hesin_oper\">,\n <Entity \"hesin_psych\">,\n <Entity \"covid19_result_england\">,\n <Entity \"covid19_result_scotland\">,\n <Entity \"covid19_result_wales\">,\n <Entity \"gp_clinical\">,\n <Entity \"gp_registrations\">,\n <Entity \"gp_scripts\">]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get the participant sql table, which provides the dxdata package with information\n",
    "to extract individual participant information."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "participant = dataset[\"participant\"]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use the participant table to pull specific data fields from the UK Biobank database. The find_field method simply takes a UKBiobank field ID (e.g. field 22001 is genetic sex). Here we need to extract several fields:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "search_fields = ['eid', 'p22001', 'p21003_i0', 'p22000']\n",
    "rename_dict = {'p22001': '22001-0.0', 'p21003_i0': '21003-0.0', 'p22000': '22000-0.0'}\n",
    "for PC in range(1,41):\n",
    "    search_fields.append(f'p22009_a{PC}')\n",
    "    rename_dict[f'p22009_a{PC}'] = f'22009-0.{PC}'\n",
    "    "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are actually extracting the per-individual values. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df = participant.retrieve_fields(names = search_fields,\n",
    "                                 coding_values = \"keep\",\n",
    "                                 engine = dxdata.connect())"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert data frame to Pandas\n",
    "df_pandas = df.toPandas()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going to get information on the WES batch for each study individual. This requires bcftools to be installed. To install bcftools in a pycharm spark instance, open a new terminal and type:\n",
    "\n",
    "```\n",
    "apt-get update\n",
    "apt-get intall bcftools\n",
    "```\n",
    "\n",
    "Also remember to change the file/project IDs below to your specific project."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Define file IDs for each VCF we need:\n",
    "vcf_200k = 'file-Fz7GfKjJfQZy0y652gXxxYJz'\n",
    "vcf_450k = 'file-G56qJV8JykJqJ3p94qzbgFbq'\n",
    "vcf_470k = 'file-G97fyZ8JykJV073Z34fYgYz9'\n",
    "\n",
    "dxpy.download_dxfile(vcf_200k, '200k_chrY.vcf.gz', project='project-GFPBQv8J0zVvBX9XJyqyqYz1')\n",
    "dxpy.download_dxfile(vcf_450k, '450k_chrY.vcf.gz', project='project-GFPBQv8J0zVvBX9XJyqyqYz1')\n",
    "dxpy.download_dxfile(vcf_470k, '470k_chrY.vcf.gz', project='project-GFPBQv8J0zVvBX9XJyqyqYz1')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for version in ['200k', '450k', '470k']:\n",
    "    cmd = f'bcftools query -l {version}_chrY.vcf.gz > {version}_samples.txt'\n",
    "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    print(cmd)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "bcftools query -l 200k_chrY.vcf.gz > 200k_samples.txt\nbcftools query -l 450k_chrY.vcf.gz > 450k_samples.txt\nbcftools query -l 470k_chrY.vcf.gz > 470k_samples.txt\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "file_470k = open('470k_samples.txt', 'r')\n",
    "samples_470k = set()\n",
    "for sample in file_470k:\n",
    "    sample = sample.rstrip()\n",
    "    samples_470k.add(sample)\n",
    "print(len(samples_470k))\n",
    "\n",
    "file_450k = open('450k_samples.txt', 'r')\n",
    "samples_450k = set()\n",
    "for sample in file_450k:\n",
    "    sample = sample.rstrip()\n",
    "    samples_450k.add(sample)\n",
    "print(len(samples_450k))\n",
    "    \n",
    "file_200k = open('200k_samples.txt', 'r')\n",
    "samples_200k = set()\n",
    "for sample in file_200k:\n",
    "    sample = sample.rstrip()\n",
    "    samples_200k.add(sample)    \n",
    "print(len(samples_200k))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "469835\n454756\n200643\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are just lightly reformatting the pandas dataframe to get it into the shape we want. See below for specific changes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Set names to the old-school versions from the UKBB file pre-DNANexus for compatability\n",
    "df_pandas = df_pandas.rename(columns = rename_dict)\n",
    "\n",
    "# Cast sex to an int. I convert to a string to force into 'int' as numpy does not have an NA int dtype\n",
    "df_pandas['22001-0.0'] = df_pandas['22001-0.0'].apply(lambda x: 'NA' if np.isnan(x) else str(int(x)))\n",
    "\n",
    "# Decide which batch an individual belongs to\n",
    "# Note that 50k is not here, yet, due to unavailability of any 50k data on the RAP\n",
    "def decide_wes(eid):\n",
    "    if eid in samples_200k:\n",
    "        return('200k')\n",
    "    elif eid in samples_450k:\n",
    "        return('450k')\n",
    "    elif eid in samples_470k:\n",
    "        return('470k')\n",
    "    else:\n",
    "        return(None)\n",
    "    \n",
    "df_pandas['wes.batch'] = df_pandas['eid'].apply(lambda x: decide_wes(x))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we read in the .fam file for genetic data we just created. This is so we can add a variable indicating whether genetic data is available or not (again, remember to update file IDs):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fam_470k = 'file-GGJb0zQJ80QbJ1fgJQpFQpK6'\n",
    "\n",
    "dxpy.download_dxfile(fam_470k, 'UKBB_470K_Autosomes_QCd.fam', project='project-GFPBQv8J0zVvBX9XJyqyqYz1')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fam_reader = csv.DictReader(open('UKBB_470K_Autosomes_QCd.fam','r'), fieldnames=['FID','IID','FATHER_ID','MOTHER_ID','sex','phenotype'], delimiter=\"\\t\")\n",
    "genetics_samples = set()\n",
    "for indv in fam_reader:\n",
    "    genetics_samples.add(indv['FID'])\n",
    "    \n",
    "# And add a column into the master covariate table indicating presence in this fam file:\n",
    "df_pandas['genetics_qc_pass'] = df_pandas['eid'].isin(genetics_samples).apply(lambda x: 1 if x is True else 0)\n",
    "df_pandas['genetics_qc_pass'].value_counts()\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    468519\n0     33890\nName: genetics_qc_pass, dtype: int64"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to convert the array batch into a categorical character value to enable easy processing during association tests."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_pandas['22000-0.0'] = df_pandas['22000-0.0'].apply(lambda x: f'axiom{x:0.0f}' if x > 0 else f'bileve{x*-1:0.0f}')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Write an output CSV file:\n",
    "df_pandas.to_csv('base_covariates.covariates', sep = \"\\t\", na_rep=\"NA\", index=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure to upload the final file from a terminal!\n",
    "dx upload base_covariates.covariates \n",
    "\n",
    "**REMEMBER** This will upload to the container. NOT the project, so be sure to copy it into your project. No idea why it works this way..."
   ],
   "metadata": {}
  }
 ]
}
