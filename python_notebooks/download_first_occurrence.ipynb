{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "Do initial setup. This involves the next four code-blocks:\n\n1. Import required modules:\n    * pyspark \u2013 allows for running on multiple nodes\n    * dxpy \u2013 allows for querying DNANexus-specific files\n    * dxdata \u2013 allows for querying DNANexus-specific databases\n    * pandas \u2013 dataframe manipulation\n2. Connect to the pyspark cluster to be able to pull UK Biobank data, structured in a mysql-like database\n3. Use dxpy to find the database 'file' in our current project.\n4. Load the database into this instance with the dxdata package", "metadata": {}}, {"cell_type": "code", "source": "import re\nimport pyspark\nimport dxpy\nimport dxdata\nimport pandas as pd\nimport numpy as np", "metadata": {"trusted": true}, "execution_count": 36, "outputs": []}, {"cell_type": "code", "source": "sc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)", "metadata": {"trusted": true}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "source": "dispensed_dataset_id = dxpy.find_one_data_object(typename = \"Dataset\",\n                                                name = \"app*.dataset\", folder = \"/\",\n                                                name_mode = \"glob\") [\"id\"]", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "dataset = dxdata.load_dataset(id = dispensed_dataset_id)\ndataset.entities", "metadata": {"trusted": true}, "execution_count": 4, "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "[<Entity \"participant\">,\n <Entity \"death\">,\n <Entity \"death_cause\">,\n <Entity \"hesin\">,\n <Entity \"hesin_critical\">,\n <Entity \"hesin_delivery\">,\n <Entity \"hesin_diag\">,\n <Entity \"hesin_maternity\">,\n <Entity \"hesin_oper\">,\n <Entity \"hesin_psych\">,\n <Entity \"covid19_result_england\">,\n <Entity \"covid19_result_scotland\">,\n <Entity \"covid19_result_wales\">,\n <Entity \"gp_clinical\">,\n <Entity \"gp_registrations\">,\n <Entity \"gp_scripts\">]"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "Now we get the participant sql table, which provides the dxdata package with information\nto extract individual participant information.", "metadata": {}}, {"cell_type": "code", "source": "participant = dataset[\"participant\"]", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": "Now use the participant table to pull specific data fields from the UK Biobank database. The `find_field`\nmethod simply takes a UKBiobank field ID (e.g. field 22001 is genetic sex). Here we need to extract several \nfields:\n* Fields 130000 - 132605 are First Incidence records (i.e. GP data).\n    * Even fields (e.g. 130000) is the data that a record was entered into a participant's medical records\n    * Odd fields (e.g. 130001) is the source of the record itself (e.g. Death Record, HES, etc.)", "metadata": {}}, {"cell_type": "code", "source": "field_names = ['eid']\ntitles = ['eid']\nfor i in range(130000,132605, 2):\n    field_name = 'p%i' % i\n    try:\n        curr_field = participant.find_field(field_name)\n        field_names.append(field_name)\n        titles.append(curr_field.title)\n    except LookupError:\n        continue\n\nprint('%s - %s' % (field_names[0], titles[0]))\nlen(field_names)", "metadata": {"trusted": true}, "execution_count": 6, "outputs": [{"name": "stdout", "text": "eid - eid\n", "output_type": "stream"}, {"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "1131"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "Here we are actually extracting the per-individual values. Due to some 'weirdness' with pyspark,\nI extract a fixed number of fields to ensure that we don't run into a memory issue. This issue \nhappens regardless of instance size.", "metadata": {}}, {"cell_type": "code", "source": "file_counter = 1\n\nfor i in range (1, len(field_names), 100):\n    end = i + 99 if (i + 100) < len(field_names) else (len(field_names) - 1)\n    print('%i - %i' % (i, end))\n    \n    curr_names = field_names[i:end]\n    curr_names.insert(0, field_names[0])\n    curr_titles = titles[i:end]\n    curr_titles.insert(0, 'eid')\n    \n    df = participant.retrieve_fields(names = curr_names,\n                                     coding_values = \"replace\",\n                                     engine = dxdata.connect())\n    \n    df_pd = df.toPandas()\n    \n    # Make a name remapper:\n    name_map = {}\n    for i in range(1,len(curr_names), 1):\n        short_name = re.search(\"[A-Z]\\d{2}\",curr_titles[i]).group(0)\n        name_map[curr_names[i]] = short_name\n        \n    df_pd = df_pd.rename(columns = name_map)\n    \n    # Cannot combine all of the files together due to memory problems, so write to individual files to be processed later.\n    df_pd.to_csv(f'first_incidence_table{file_counter}.tsv', sep = \"\\t\", na_rep='NA', index = F)\n    \n    file_counter += 1", "metadata": {"trusted": true}, "execution_count": null, "outputs": [{"name": "stdout", "text": "1 - 100\n101 - 200\n201 - 300\n301 - 400\n401 - 500\n501 - 600\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "Also need to download participant birth / death information so we can do time to event-type analyses", "metadata": {}}, {"cell_type": "code", "source": "df = participant.retrieve_fields(names = ['eid','p52','p34','p40000_i0'],\n                                 coding_values = \"replace\",\n                                 engine = dxdata.connect())", "metadata": {"trusted": true}, "execution_count": 72, "outputs": []}, {"cell_type": "code", "source": "df_pd = df.toPandas()\n\n# Convert to actual dob/dod\ndf_pd = df_pd[pd.isnull(df_pd['p52'])==False]\ndf_pd['dob'] = df_pd.apply(lambda x: pd.to_datetime(f'{x[\"p34\"]:0.0f}-{x[\"p52\"]}-15', format='%Y-%B-%d'), axis=1)\ndf_pd = df_pd.drop(columns={'p52','p34'})\ndf_pd = df_pd.rename(columns={'p40000_i0':'dod'})\n\n# Write out...\ndf_pd.to_csv('vital_stats.tsv', sep=\"\\t\", index=False, na_rep=\"NA\")", "metadata": {"trusted": true}, "execution_count": 75, "outputs": []}]}